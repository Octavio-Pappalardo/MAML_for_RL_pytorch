{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO ES CODIGO TERMINADO. Lo dejo por si quiero comparar con algo o sacar ideas pero no esta chequeado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTIONAL IMPLEMENTATION \n",
    "Using torch.func\n",
    "\n",
    "Parece ser que usar torch.func no sirve tanto para este contexto porque la paralelizacion de vmap no se lleva bien con collectar data de distintos envs ; esta pensado mas para parelizar tensor operations ( Si vendria bien para MAML en el supervised setting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.func import functional_call, grad ,vmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_data_from_env(params ,env,num_steps):\n",
    "    #episode_return=0\n",
    "    episodes_lengths=[]\n",
    "    episodes_returns=[]\n",
    "\n",
    "    #instantiate a buffer to save the data in\n",
    "    buffer= Data_buffer(num_steps=num_steps ,env=env ,device=device)\n",
    "    \n",
    "    #get an initial state from the agent \n",
    "    #next_obs = torch.tensor(env.reset()[0],dtype=torch.float32).to(device)\n",
    "    next_obs=env.reset()[0] #!!!\n",
    "    done = torch.zeros(1).to(device)\n",
    "\n",
    "    for step in range(0, num_steps):\n",
    "\n",
    "        #prepare for new step: next_obs is the new step obs , the done flag which indicates wether the episode finished in the\n",
    "        #last step becomes this episodes prev_done.\n",
    "        obs, prev_done = next_obs, done\n",
    "\n",
    "        obs=minigrid_preprocess_obs(obs).to(device) #!!! ---\n",
    "        # get actionA action predictions and state value estimates\n",
    "        with torch.no_grad():\n",
    "            action, logprob, _, value = functional_call(agent,params,obs.unsqueeze(0))  #policy.get_action_and_value(obs.unsqueeze(0))\n",
    "\n",
    "        #execute the action and get environment response.\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action.cpu().numpy())  #!!! - action.squeeze(0).cpu().numpy()\n",
    "        done= torch.max(torch.tensor([terminated,truncated],dtype=torch.float32))#int(terminated or truncated)\n",
    "\n",
    "        #preprocess and store data that you get from a step in a standard RL algo\n",
    "        reward= torch.as_tensor(reward,dtype=torch.float32).to(device)\n",
    "\n",
    "        buffer.store_step_data(step_index=step, obs=obs, act=action.long(), reward=reward, val=value\n",
    "                                                        , logp=logprob,prev_done=prev_done)\n",
    "\n",
    "\n",
    "        #prepare for next step\n",
    "        #next_obs, done =torch.as_tensor(next_obs,dtype=torch.float32).to(device) ,torch.as_tensor(done,dtype=torch.float32).to(device)\n",
    "        done = torch.as_tensor(done,dtype=torch.float32).to(device)\n",
    "\n",
    "        #global_step += 1 \n",
    "        #episode_step_num+=1 ## just for metaworld\n",
    "        #episode_return+= reward\n",
    "        \n",
    "        #deal with the case where the episode ends for  the environment\n",
    "        if done:\n",
    "            #reset environment\n",
    "            next_obs = env.reset()[0] #!! - next_obs = torch.Tensor(env.reset()[0]).to(device)\n",
    "            \n",
    "            assert 'episode' in info , 'problem with recordeEpisodeStatistics wrapper'\n",
    "            episodes_returns.append(info['episode']['r'][0])\n",
    "            episodes_lengths.append(info['episode']['l'][0])\n",
    "            #if len(episodes_lengths)%10==0:\n",
    "            #if global_step>print_info_factor*5000:\n",
    "            #    print(f\"global_step={global_step}, episodic_return={info['episode']['r'][0]} ,episodic_length={info['episode']['l'][0]} \")\n",
    "            #    print_info_factor+=1\n",
    "            \n",
    "        \n",
    "    # bootstrap value if not done\n",
    "    with torch.no_grad():\n",
    "        #get state value estimate of the new state\n",
    "        processed_next_obs=minigrid_preprocess_obs(next_obs).to(device)\n",
    "        _, _ , _, next_value = functional_call(agent,params,processed_next_obs.unsqueeze(0))\n",
    "        #next_value = agent.get_value(processed_next_obs.unsqueeze(0))\n",
    "        #next_value = policy.get_value(next_obs)\n",
    "        \n",
    "        #calculate the advantages and to go returns for each state visited in the data. They get stored in the buffer automatically\n",
    "        buffer.calculate_returns_and_advantages(done,next_value,gae=config.il_gae,gamma=config.il_gamma,gae_lambda=config.il_gae_lambda)\n",
    "\n",
    "    #calculate some metrics for future logging\n",
    "    #print(np.array(episodes_returns).mean() ,np.array(episodes_lengths).mean())\n",
    "    mean_episode_return= np.array(episodes_returns).mean()\n",
    "\n",
    "    return buffer , mean_episode_return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def policy_loss(params,buffer,loss_type='vanilla'):\n",
    "    '''Compute a pseudo loss from collected data such that the gradient of the pseudo loss\n",
    "    coincides with the gradient of the true RL objective\n",
    "\n",
    "    Args:\n",
    "        buffer (_type_): _description_\n",
    "        policy (_type_): _description_\n",
    "    '''\n",
    "    if loss_type== 'vanilla':\n",
    "        #obs, act, adv, logp_old = buffer.observations.reshape((-1,) + env.observation_space['image'].shape), buffer.actions.reshape((-1,) + env.action_space.shape), data['adv'], data['logp']\n",
    "        obs, act, adv, logp_old = buffer.observations , buffer.actions ,buffer.advantages, buffer.logprob_actions\n",
    "\n",
    "        #print(buffer.observations.shape , buffer.observations.reshape((-1,) + env.observation_space['image'].shape).shape ,buffer.logprob_actions.shape, buffer.logprob_actions.reshape(-1).shape)\n",
    "\n",
    "        # Policy loss\n",
    "        _ , newlogprob, entropy, newvalue = functional_call(agent,params,(obs, act) )\n",
    "        #_, newlogprob, entropy, newvalue = agent.get_action_and_value(obs, act)\n",
    "        normalize_advantage=True\n",
    "        if normalize_advantage==True:\n",
    "            adv = (adv - adv.mean()) / (adv.std() + 1e-8)\n",
    "\n",
    "        pg_loss = -(newlogprob * adv).mean()\n",
    "\n",
    "        # Useful extra info\n",
    "        with torch.no_grad():\n",
    "            approx_kl = (logp_old - newlogprob).mean().item()\n",
    "\n",
    "        #value loss\n",
    "        v_loss = 0.5 * ((newvalue - buffer.returns) ** 2).mean()\n",
    "\n",
    "        il_pseudo_policy_loss = pg_loss  + v_loss * config.il_valuef_coef\n",
    "            \n",
    "\n",
    "        return il_pseudo_policy_loss \n",
    "    \n",
    "    #if loss_type=='ppo':\n",
    "\n",
    "def policy_loss_for_adaptation(params,buffer,loss_type='vanilla'):\n",
    "    return policy_loss(params,buffer,loss_type='vanilla')\n",
    "\n",
    "def policy_loss_for_meta_update(params,buffer,loss_type='vanilla'):\n",
    "    return policy_loss(params,buffer,loss_type='vanilla')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptation_inner_loop(base_params ,env ,logger=None):\n",
    "    \n",
    "    \n",
    "    # collect data with base policy for doing the inner loop adaptation\n",
    "\n",
    "    buffer, mean_episode_return= collect_data_from_env(params=base_params ,env=env, num_steps=config.env_steps_to_estimate_loss)\n",
    "    if logger:\n",
    "        logger.base_policy_episode_returns.append(mean_episode_return)\n",
    "    \n",
    "    # Compute gradient of the tasks's policy objective function (with data collected with the base policy)\n",
    "    # with respect to the base policy parameters\n",
    "    grads = grad(policy_loss_for_adaptation,argnums=0, has_aux=False)(base_params ,il_buffer)\n",
    "\n",
    "    updated_params = OrderedDict()\n",
    "    for (name, param), grad in zip(base_params.items(), grads):\n",
    "        updated_params[name] = param - config.il_lr * grad\n",
    "    \n",
    "\n",
    "    # collect data with adapted policy for the outer loop update\n",
    "    buffer,mean_episode_return =collect_data_from_env(params=updated_params ,env=env, num_steps=config.env_steps_to_estimate_loss)\n",
    "    if logger:\n",
    "        logger.adapted_policies_episode_returns.append(mean_episode_return)\n",
    "    # Compute gradient of the tasks's policy objective function (with data collected with the adapted policy- the policy objective evaluated at the updated policy)\n",
    "    # with respect to the base policy parameters\n",
    "    adapted_pg_loss, adapted_v_loss = policy_loss_for_meta_update(params=updated_params ,buffer=buffer)\n",
    "    meta_pseudo_policy_loss = adapted_pg_loss  + adapted_v_loss * config.il_valuef_coef\n",
    "\n",
    "    return meta_pseudo_policy_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_loop():\n",
    "\n",
    "    for i in range(config.num_meta_updates):\n",
    "        meta_loss=0\n",
    "        envs=[sample_env(size=env_size ,max_steps=max_episode_length) for _ in range(config.num_tasks_per_meta_update)]\n",
    "        for env in envs:\n",
    "            meta_loss+= adaptation_inner_loop(agent_model=agent, env=env, logger=logger)\n",
    "            grads = grad(adaptation_inner_loop,argnums=0, has_aux=False)(agent.parameters() ,il_buffer)\n",
    "\n",
    "        #grads = torch.autograd.grad(meta_loss,\n",
    "        #                    agent.parameters())#,retain_graph=True)\n",
    "            \n",
    "        meta_optimizer.zero_grad()\n",
    "        meta_loss.backward()\n",
    "        meta_optimizer.step()\n",
    "        \n",
    "        print(f'completed meta update {i}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def outer_loop():\n",
    "\n",
    "    vmap(grad(adaptation_inner_loop)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func1(a):\n",
    "    env=sample_env(8,100)\n",
    "    obs=func(env)\n",
    "    return obs\n",
    "def func(env):\n",
    "    obs=env.reset()[0]\n",
    "    obs=minigrid_preprocess_obs(obs)\n",
    "    random_action_index = torch.randint(0, 6, (1,)).numpy()\n",
    "    next_obs, reward, terminated, truncated, info = env.step(random_action_index)\n",
    "    next_obs=minigrid_preprocess_obs(next_obs)\n",
    "    return next_obs\n",
    "\n",
    "envs=[sample_env(8,100) for i in range(8)]\n",
    "a=torch.randn(8,10)\n",
    "results=vmap(func1,randomness=\"different\")(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
